{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i2fvbP5EFQn"
      },
      "source": [
        "Data collection for AI inequality research.\n",
        "\n",
        "Sources:\n",
        "- Bureau of Labor Statistics data\n",
        "- AI adoption rates across industries from multiple sources\n",
        "- Economic indicators and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmOZ7HqdD28f"
      },
      "source": [
        "#### 1. Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0ho-nspCQW6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Imports and setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcvWY2a6Ix5T"
      },
      "source": [
        "#### 3 Scraping Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xTAjxezE5ym"
      },
      "source": [
        "##### 3.1 Scraping Labor Statistics API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvt47G7nDahr"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL: BLS Data Scraping - Standalone\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Bureau of Labor Statistics (BLS) Data Scraper\n",
        "\n",
        "This standalone script scrapes employment data from the BLS API.\n",
        "It retrieves employment statistics, wage data, and occupational information\n",
        "without the overhead of class-based architecture.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from typing import List, Optional\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def scrape_bls_employment_data(series_ids: List[str], api_key: Optional[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape employment data from BLS API for specified series IDs.\n",
        "\n",
        "    Args:\n",
        "        series_ids: List of BLS series IDs to retrieve (e.g., ['CES0000000001', 'CES0500000003'])\n",
        "        api_key: BLS API key (optional for public data)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing employment data with columns: series_id, year, period, value, footnotes\n",
        "    \"\"\"\n",
        "    base_url = \"https://api.bls.gov/publicAPI/v2\"\n",
        "\n",
        "    # Prepare the request payload\n",
        "    payload = {\n",
        "        \"seriesid\": series_ids,\n",
        "        \"startyear\": \"2020\",\n",
        "        \"endyear\": str(datetime.now().year),\n",
        "        \"registrationkey\": api_key if api_key else \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Requesting BLS data for series: {series_ids}\")\n",
        "\n",
        "        # Make the API request\n",
        "        response = requests.post(f\"{base_url}/timeseries/data/\", json=payload)\n",
        "        response.raise_for_status()  # Raise exception for bad status codes\n",
        "\n",
        "        # Parse the JSON response\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get('status') == 'REQUEST_SUCCEEDED':\n",
        "            # Extract data from the response\n",
        "            results = []\n",
        "            for series in data.get('Results', {}).get('series', []):\n",
        "                series_id = series.get('seriesID', '')\n",
        "\n",
        "                for item in series.get('data', []):\n",
        "                    results.append({\n",
        "                        'series_id': series_id,\n",
        "                        'year': item.get('year', ''),\n",
        "                        'period': item.get('period', ''),\n",
        "                        'periodName': item.get('periodName', ''),\n",
        "                        'value': item.get('value', ''),\n",
        "                        'footnotes': [note.get('text', '') for note in item.get('footnotes', [])]\n",
        "                    })\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            df = pd.DataFrame(results)\n",
        "\n",
        "            # Convert value column to numeric, handling 'null' strings\n",
        "            df['value'] = pd.to_numeric(df['value'].replace('null', pd.NA), errors='coerce')\n",
        "\n",
        "            logger.info(f\"Successfully retrieved {len(df)} data points\")\n",
        "            return df\n",
        "\n",
        "        else:\n",
        "            logger.error(f\"BLS API request failed: {data.get('message', 'Unknown error')}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Network error when requesting BLS data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except json.JSONDecodeError as e:\n",
        "        logger.error(f\"Error parsing BLS API response: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def get_common_bls_series() -> dict:\n",
        "    \"\"\"\n",
        "    Get a dictionary of common BLS series IDs with their descriptions.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping series IDs to descriptions\n",
        "    \"\"\"\n",
        "    return {\n",
        "        # Total Nonfarm Employment\n",
        "        'CES0000000001': 'Total Nonfarm Employment',\n",
        "\n",
        "        # Private Sector Employment\n",
        "        'CES0500000003': 'Private Sector Employment',\n",
        "\n",
        "        # Manufacturing Employment\n",
        "        'CES3000000001': 'Manufacturing Employment',\n",
        "\n",
        "        # Information Technology Employment\n",
        "        'CES5000000001': 'Information Services Employment',\n",
        "\n",
        "        # Professional and Business Services\n",
        "        'CES6000000001': 'Professional and Business Services Employment',\n",
        "\n",
        "        # Average Hourly Earnings - Private Sector\n",
        "        'CES0500000003': 'Average Hourly Earnings - Private Sector',\n",
        "\n",
        "        # Average Weekly Hours - Private Sector\n",
        "        'CES0500000002': 'Average Weekly Hours - Private Sector'\n",
        "    }\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Get common series IDs\n",
        "    common_series = get_common_bls_series()\n",
        "    print(\"Common BLS Series IDs:\")\n",
        "    for series_id, description in common_series.items():\n",
        "        print(f\"  {series_id}: {description}\")\n",
        "\n",
        "    # Example: Scrape data for total nonfarm employment\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Scraping BLS Employment Data...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # You can replace these with your own series IDs\n",
        "    test_series = ['CES0000000001', 'CES3000000001']  # Total nonfarm and manufacturing\n",
        "\n",
        "    # Scrape the data\n",
        "    employment_df = scrape_bls_employment_data(test_series)\n",
        "\n",
        "    if not employment_df.empty:\n",
        "        print(f\"\\nRetrieved {len(employment_df)} data points\")\n",
        "        print(\"\\nFirst 10 rows of data:\")\n",
        "        print(employment_df.head(10))\n",
        "\n",
        "        # Basic statistics\n",
        "        print(f\"\\nData summary:\")\n",
        "        print(f\"Date range: {employment_df['year'].min()} - {employment_df['year'].max()}\")\n",
        "        print(f\"Series IDs: {employment_df['series_id'].unique()}\")\n",
        "\n",
        "        # Save to CSV (optional)\n",
        "        # employment_df.to_csv('bls_employment_data.csv', index=False)\n",
        "        # print(\"\\nData saved to 'bls_employment_data.csv'\")\n",
        "\n",
        "    else:\n",
        "        print(\"No data retrieved. Check your series IDs and internet connection.\")\n",
        "\n",
        "print(\"BLS scraping script ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcIylXuQGIhP"
      },
      "source": [
        "##### 3.2 Scraping AI Adoption Trackers from Websites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MtVjUBeIUhs"
      },
      "source": [
        "###### Scraping per source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2jP6DlgDeZh"
      },
      "outputs": [],
      "source": [
        "# SPECIFIC SCRAPING\n",
        "\n",
        "\"\"\"\n",
        "AI Adoption Data Scraper - Real Data Collection\n",
        "\n",
        "This script scrapes actual AI adoption data from various sources including\n",
        "research reports, surveys, and public datasets. It collects real metrics\n",
        "on AI adoption rates across different industries and time periods.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def scrape_mckinsey_ai_adoption() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape AI adoption data from McKinsey Global Institute reports.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing AI adoption data from McKinsey sources\n",
        "    \"\"\"\n",
        "    logger.info(\"Scraping McKinsey AI adoption data...\")\n",
        "\n",
        "    # McKinsey AI adoption data sources\n",
        "    mckinsey_urls = [\n",
        "        \"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year\",\n",
        "        \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review\"\n",
        "    ]\n",
        "\n",
        "    adoption_data = []\n",
        "\n",
        "    for url in mckinsey_urls:\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract year from URL or content\n",
        "            year_match = re.search(r'202[0-9]', url)\n",
        "            year = int(year_match.group()) if year_match else datetime.now().year\n",
        "\n",
        "            # Look for adoption statistics in the content\n",
        "            # This is a simplified approach - in practice you'd need more sophisticated parsing\n",
        "            text_content = soup.get_text()\n",
        "\n",
        "            # Common patterns for AI adoption data\n",
        "            adoption_patterns = [\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*percent.*adopt.*AI',\n",
        "                r'AI.*adoption.*(\\d+(?:\\.\\d+)?)\\s*percent',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*organizations.*AI',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*companies.*AI'\n",
        "            ]\n",
        "\n",
        "            for pattern in adoption_patterns:\n",
        "                matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        adoption_rate = float(match) / 100  # Convert percentage to decimal\n",
        "                        adoption_data.append({\n",
        "                            'source': 'mckinsey_global_institute',\n",
        "                            'year': year,\n",
        "                            'adoption_rate': adoption_rate,\n",
        "                            'industry': 'cross_industry',\n",
        "                            'data_type': 'survey',\n",
        "                            'url': url\n",
        "                        })\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "            time.sleep(1)  # Be respectful to the server\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(adoption_data)\n",
        "\n",
        "def scrape_gartner_ai_adoption() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape AI adoption data from Gartner research reports.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing AI adoption data from Gartner sources\n",
        "    \"\"\"\n",
        "    logger.info(\"Scraping Gartner AI adoption data...\")\n",
        "\n",
        "    # Gartner AI adoption data sources\n",
        "    gartner_urls = [\n",
        "        \"https://www.gartner.com/en/newsroom/press-releases/2023-08-15-gartner-identifies-four-emerging-technologies-that-will-transform-business-outcomes\",\n",
        "        \"https://www.gartner.com/en/newsroom/press-releases/2022-08-16-gartner-identifies-four-emerging-technologies-that-will-transform-business-outcomes\"\n",
        "    ]\n",
        "\n",
        "    adoption_data = []\n",
        "\n",
        "    for url in gartner_urls:\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract year from URL or content\n",
        "            year_match = re.search(r'202[0-9]', url)\n",
        "            year = int(year_match.group()) if year_match else datetime.now().year\n",
        "\n",
        "            text_content = soup.get_text()\n",
        "\n",
        "            # Look for AI adoption patterns in Gartner content\n",
        "            adoption_patterns = [\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*percent.*AI.*adoption',\n",
        "                r'AI.*adoption.*(\\d+(?:\\.\\d+)?)\\s*percent',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*enterprises.*AI',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*organizations.*AI'\n",
        "            ]\n",
        "\n",
        "            for pattern in adoption_patterns:\n",
        "                matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        adoption_rate = float(match) / 100\n",
        "                        adoption_data.append({\n",
        "                            'source': 'gartner_research',\n",
        "                            'year': year,\n",
        "                            'adoption_rate': adoption_rate,\n",
        "                            'industry': 'cross_industry',\n",
        "                            'data_type': 'research',\n",
        "                            'url': url\n",
        "                        })\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(adoption_data)\n",
        "\n",
        "def scrape_world_economic_forum_ai_data() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape AI adoption data from World Economic Forum reports.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing AI adoption data from WEF sources\n",
        "    \"\"\"\n",
        "    logger.info(\"Scraping World Economic Forum AI adoption data...\")\n",
        "\n",
        "    # WEF AI adoption data sources\n",
        "    wef_urls = [\n",
        "        \"https://www.weforum.org/agenda/2023/01/ai-adoption-business-2023/\",\n",
        "        \"https://www.weforum.org/agenda/2022/01/artificial-intelligence-adoption-business/\"\n",
        "    ]\n",
        "\n",
        "    adoption_data = []\n",
        "\n",
        "    for url in wef_urls:\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract year from URL\n",
        "            year_match = re.search(r'202[0-9]', url)\n",
        "            year = int(year_match.group()) if year_match else datetime.now().year\n",
        "\n",
        "            text_content = soup.get_text()\n",
        "\n",
        "            # Look for AI adoption patterns in WEF content\n",
        "            adoption_patterns = [\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*percent.*AI.*adoption',\n",
        "                r'AI.*adoption.*(\\d+(?:\\.\\d+)?)\\s*percent',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*businesses.*AI',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*companies.*AI'\n",
        "            ]\n",
        "\n",
        "            for pattern in adoption_patterns:\n",
        "                matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        adoption_rate = float(match) / 100\n",
        "                        adoption_data.append({\n",
        "                            'source': 'world_economic_forum',\n",
        "                            'year': year,\n",
        "                            'adoption_rate': adoption_rate,\n",
        "                            'industry': 'cross_industry',\n",
        "                            'data_type': 'survey',\n",
        "                            'url': url\n",
        "                        })\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(adoption_data)\n",
        "\n",
        "def scrape_github_ai_adoption_data() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape AI adoption data from GitHub's State of the Octoverse reports.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing AI adoption data from GitHub sources\n",
        "    \"\"\"\n",
        "    logger.info(\"Scraping GitHub AI adoption data...\")\n",
        "\n",
        "    # GitHub State of the Octoverse URLs\n",
        "    github_urls = [\n",
        "        \"https://octoverse.github.com/2023/state-of-open-source\",\n",
        "        \"https://octoverse.github.com/2022/state-of-open-source\"\n",
        "    ]\n",
        "\n",
        "    adoption_data = []\n",
        "\n",
        "    for url in github_urls:\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract year from URL\n",
        "            year_match = re.search(r'202[0-9]', url)\n",
        "            year = int(year_match.group()) if year_match else datetime.now().year\n",
        "\n",
        "            text_content = soup.get_text()\n",
        "\n",
        "            # Look for AI/ML adoption patterns in GitHub content\n",
        "            adoption_patterns = [\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*percent.*AI.*ML',\n",
        "                r'AI.*ML.*(\\d+(?:\\.\\d+)?)\\s*percent',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*machine.*learning',\n",
        "                r'(\\d+(?:\\.\\d+)?)\\s*%.*artificial.*intelligence'\n",
        "            ]\n",
        "\n",
        "            for pattern in adoption_patterns:\n",
        "                matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        adoption_rate = float(match) / 100\n",
        "                        adoption_data.append({\n",
        "                            'source': 'github_octoverse',\n",
        "                            'year': year,\n",
        "                            'adoption_rate': adoption_rate,\n",
        "                            'industry': 'technology',\n",
        "                            'data_type': 'open_source',\n",
        "                            'url': url\n",
        "                        })\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(adoption_data)\n",
        "\n",
        "def collect_all_ai_adoption_data() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Collect AI adoption data from all available sources.\n",
        "\n",
        "    Returns:\n",
        "        Combined DataFrame with all AI adoption data\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting comprehensive AI adoption data collection...\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    # Collect data from all sources\n",
        "    sources = [\n",
        "        scrape_mckinsey_ai_adoption,\n",
        "        scrape_gartner_ai_adoption,\n",
        "        scrape_world_economic_forum_ai_data,\n",
        "        scrape_github_ai_adoption_data\n",
        "    ]\n",
        "\n",
        "    for source_func in sources:\n",
        "        try:\n",
        "            df = source_func()\n",
        "            if not df.empty:\n",
        "                all_data.append(df)\n",
        "                logger.info(f\"Collected {len(df)} records from {source_func.__name__}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error collecting data from {source_func.__name__}: {e}\")\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        logger.info(f\"Total records collected: {len(combined_df)}\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        logger.warning(\"No data collected from any source\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def save_ai_adoption_data(df: pd.DataFrame, filename: str = \"ai_adoption_data.csv\") -> None:\n",
        "    \"\"\"\n",
        "    Save AI adoption data to CSV file.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing AI adoption data\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df.to_csv(filename, index=False)\n",
        "        logger.info(f\"AI adoption data saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving data: {e}\")\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"AI ADOPTION DATA SCRAPER - REAL DATA COLLECTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Collect all AI adoption data\n",
        "    print(\"\\nCollecting AI adoption data from multiple sources...\")\n",
        "    ai_adoption_df = collect_all_ai_adoption_data()\n",
        "\n",
        "    if not ai_adoption_df.empty:\n",
        "        print(f\"\\n✓ Successfully collected {len(ai_adoption_df)} data points\")\n",
        "        print(f\"✓ Sources: {ai_adoption_df['source'].unique()}\")\n",
        "        print(f\"✓ Years: {sorted(ai_adoption_df['year'].unique())}\")\n",
        "        print(f\"✓ Industries: {ai_adoption_df['industry'].unique()}\")\n",
        "\n",
        "        # Show sample data\n",
        "        print(\"\\nSample data collected:\")\n",
        "        print(ai_adoption_df.head(10))\n",
        "\n",
        "        # Summary statistics\n",
        "        print(f\"\\nSummary Statistics:\")\n",
        "        print(f\"   Average adoption rate: {ai_adoption_df['adoption_rate'].mean():.1%}\")\n",
        "        print(f\"   Highest adoption rate: {ai_adoption_df['adoption_rate'].max():.1%}\")\n",
        "        print(f\"   Lowest adoption rate: {ai_adoption_df['adoption_rate'].min():.1%}\")\n",
        "\n",
        "        # Data by source\n",
        "        print(f\"\\nData by source:\")\n",
        "        source_summary = ai_adoption_df.groupby('source').agg({\n",
        "            'adoption_rate': ['count', 'mean', 'std']\n",
        "        }).round(3)\n",
        "        print(source_summary)\n",
        "\n",
        "        # Save the data\n",
        "        print(f\"\\nSaving data...\")\n",
        "        save_ai_adoption_data(ai_adoption_df)\n",
        "\n",
        "    else:\n",
        "        print(\"✗ No data collected. This might be due to:\")\n",
        "        print(\"   - Network connectivity issues\")\n",
        "        print(\"   - Website structure changes\")\n",
        "        print(\"   - Rate limiting by websites\")\n",
        "        print(\"   - Need for more sophisticated scraping techniques\")\n",
        "\n",
        "print(\"\\nAI Adoption Data Scraper ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6W4-sE4IeWa"
      },
      "source": [
        "###### Universal Scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVSykgwGIGUR"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL: Universal AI Adoption Data Scraper\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Universal AI Adoption Data Scraper\n",
        "\n",
        "This script can scrape AI adoption data from any URL by using intelligent\n",
        "pattern matching to detect adoption rates, percentages, and statistics\n",
        "without needing separate parsers for each website.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import urllib.parse\n",
        "\n",
        "def extract_year_from_url_or_content(url: str, soup: BeautifulSoup) -> int:\n",
        "    \"\"\"\n",
        "    Extract year from URL or webpage content.\n",
        "\n",
        "    Args:\n",
        "        url: The webpage URL\n",
        "        soup: BeautifulSoup object of the page\n",
        "\n",
        "    Returns:\n",
        "        Year as integer, defaults to current year if not found\n",
        "    \"\"\"\n",
        "    # Try to extract year from URL first\n",
        "    year_match = re.search(r'20[12]\\d', url)  # Matches 2010-2029\n",
        "    if year_match:\n",
        "        return int(year_match.group())\n",
        "\n",
        "    # Try to extract year from page content\n",
        "    text_content = soup.get_text()\n",
        "    year_patterns = [\n",
        "        r'20[12]\\d',  # Any year 2010-2029\n",
        "        r'published.*20[12]\\d',  # Published in year\n",
        "        r'updated.*20[12]\\d',    # Updated in year\n",
        "        r'©.*20[12]\\d'           # Copyright year\n",
        "    ]\n",
        "\n",
        "    for pattern in year_patterns:\n",
        "        matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "        if matches:\n",
        "            return int(matches[0])\n",
        "\n",
        "    return datetime.now().year\n",
        "\n",
        "def extract_ai_adoption_patterns(text_content: str) -> List[Tuple[float, str, str]]:\n",
        "    \"\"\"\n",
        "    Extract AI adoption rates using comprehensive pattern matching.\n",
        "\n",
        "    Args:\n",
        "        text_content: The text content of the webpage\n",
        "\n",
        "    Returns:\n",
        "        List of tuples: (adoption_rate, context, pattern_used)\n",
        "    \"\"\"\n",
        "    adoption_data = []\n",
        "\n",
        "    # Comprehensive patterns for AI adoption rates\n",
        "    patterns = [\n",
        "        # Direct percentage patterns\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*AI.*adoption', 'percent_ai_adoption'),\n",
        "        (r'AI.*adoption.*(\\d+(?:\\.\\d+)?)\\s*percent', 'ai_adoption_percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*organizations.*AI', 'percent_orgs_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*companies.*AI', 'percent_companies_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*businesses.*AI', 'percent_businesses_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*enterprises.*AI', 'percent_enterprises_ai'),\n",
        "\n",
        "        # Adoption rate patterns\n",
        "        (r'adoption.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'adoption_rate_percent'),\n",
        "        (r'adoption.*rate.*(\\d+(?:\\.\\d+)?)\\s*%', 'adoption_rate_percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*adoption.*rate', 'percent_adoption_rate'),\n",
        "\n",
        "        # Implementation patterns\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*implement.*AI', 'percent_implement_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*implement.*AI', 'percent_implement_ai'),\n",
        "        (r'AI.*implementation.*(\\d+(?:\\.\\d+)?)\\s*percent', 'ai_implementation_percent'),\n",
        "\n",
        "        # Usage patterns\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*use.*AI', 'percent_use_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*use.*AI', 'percent_use_ai'),\n",
        "        (r'AI.*usage.*(\\d+(?:\\.\\d+)?)\\s*percent', 'ai_usage_percent'),\n",
        "\n",
        "        # Deployment patterns\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*deploy.*AI', 'percent_deploy_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*deploy.*AI', 'percent_deploy_ai'),\n",
        "        (r'AI.*deployment.*(\\d+(?:\\.\\d+)?)\\s*percent', 'ai_deployment_percent'),\n",
        "\n",
        "        # Machine learning specific patterns\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*machine.*learning', 'percent_ml'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*%.*machine.*learning', 'percent_ml'),\n",
        "        (r'machine.*learning.*(\\d+(?:\\.\\d+)?)\\s*percent', 'ml_percent'),\n",
        "\n",
        "        # Industry-specific patterns\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*tech.*companies.*AI', 'percent_tech_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*finance.*AI', 'percent_finance_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*healthcare.*AI', 'percent_healthcare_ai'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*manufacturing.*AI', 'percent_manufacturing_ai'),\n",
        "\n",
        "        # Survey and study patterns\n",
        "        (r'survey.*(\\d+(?:\\.\\d+)?)\\s*percent.*AI', 'survey_percent_ai'),\n",
        "        (r'study.*(\\d+(?:\\.\\d+)?)\\s*percent.*AI', 'study_percent_ai'),\n",
        "        (r'research.*(\\d+(?:\\.\\d+)?)\\s*percent.*AI', 'research_percent_ai'),\n",
        "\n",
        "        # Growth and increase patterns\n",
        "        (r'AI.*adoption.*increased.*(\\d+(?:\\.\\d+)?)\\s*percent', 'ai_adoption_increase'),\n",
        "        (r'AI.*adoption.*grew.*(\\d+(?:\\.\\d+)?)\\s*percent', 'ai_adoption_growth'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*increase.*AI', 'percent_increase_ai'),\n",
        "    ]\n",
        "\n",
        "    for pattern, pattern_type in patterns:\n",
        "        matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                adoption_rate = float(match) / 100  # Convert percentage to decimal\n",
        "\n",
        "                # Get context (surrounding text)\n",
        "                match_index = text_content.lower().find(match.lower())\n",
        "                if match_index != -1:\n",
        "                    start = max(0, match_index - 100)\n",
        "                    end = min(len(text_content), match_index + 100)\n",
        "                    context = text_content[start:end].strip()\n",
        "                else:\n",
        "                    context = \"Context not found\"\n",
        "\n",
        "                adoption_data.append((adoption_rate, context, pattern_type))\n",
        "\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return adoption_data\n",
        "\n",
        "def extract_industry_from_url_or_content(url: str, soup: BeautifulSoup) -> str:\n",
        "    \"\"\"\n",
        "    Extract industry information from URL or content.\n",
        "\n",
        "    Args:\n",
        "        url: The webpage URL\n",
        "        soup: BeautifulSoup object of the page\n",
        "\n",
        "    Returns:\n",
        "        Industry name or 'cross_industry' if not specific\n",
        "    \"\"\"\n",
        "    # Check URL for industry keywords\n",
        "    url_lower = url.lower()\n",
        "\n",
        "    industry_keywords = {\n",
        "        'tech': 'technology',\n",
        "        'finance': 'finance',\n",
        "        'banking': 'finance',\n",
        "        'healthcare': 'healthcare',\n",
        "        'medical': 'healthcare',\n",
        "        'manufacturing': 'manufacturing',\n",
        "        'retail': 'retail',\n",
        "        'ecommerce': 'retail',\n",
        "        'education': 'education',\n",
        "        'energy': 'energy',\n",
        "        'automotive': 'automotive',\n",
        "        'transportation': 'transportation'\n",
        "    }\n",
        "\n",
        "    for keyword, industry in industry_keywords.items():\n",
        "        if keyword in url_lower:\n",
        "            return industry\n",
        "\n",
        "    # Check page content for industry mentions\n",
        "    text_content = soup.get_text().lower()\n",
        "    for keyword, industry in industry_keywords.items():\n",
        "        if keyword in text_content:\n",
        "            return industry\n",
        "\n",
        "    return 'cross_industry'\n",
        "\n",
        "def scrape_ai_adoption_from_url(url: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape AI adoption data from any given URL.\n",
        "\n",
        "    Args:\n",
        "        url: The URL to scrape\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing AI adoption data from the URL\n",
        "    \"\"\"\n",
        "    print(f\"Scraping: {url}\")\n",
        "\n",
        "    try:\n",
        "        # Set up headers to mimic a real browser\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "\n",
        "        # Make the request\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the HTML\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract text content (remove scripts, styles, etc.)\n",
        "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        text_content = soup.get_text()\n",
        "\n",
        "        # Extract metadata\n",
        "        year = extract_year_from_url_or_content(url, soup)\n",
        "        industry = extract_industry_from_url_or_content(url, soup)\n",
        "\n",
        "        # Extract AI adoption patterns\n",
        "        adoption_patterns = extract_ai_adoption_patterns(text_content)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        adoption_data = []\n",
        "        for adoption_rate, context, pattern_type in adoption_patterns:\n",
        "            adoption_data.append({\n",
        "                'url': url,\n",
        "                'year': year,\n",
        "                'industry': industry,\n",
        "                'adoption_rate': adoption_rate,\n",
        "                'pattern_type': pattern_type,\n",
        "                'context': context[:200] + '...' if len(context) > 200 else context,\n",
        "                'source_domain': urllib.parse.urlparse(url).netloc\n",
        "            })\n",
        "\n",
        "        if adoption_data:\n",
        "            print(f\"  Found {len(adoption_data)} adoption rate(s)\")\n",
        "        else:\n",
        "            print(f\"  No adoption rates found\")\n",
        "\n",
        "        return pd.DataFrame(adoption_data)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  Error: Network issue - {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def scrape_multiple_urls(url_list: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape AI adoption data from a list of URLs.\n",
        "\n",
        "    Args:\n",
        "        url_list: List of URLs to scrape\n",
        "\n",
        "    Returns:\n",
        "        Combined DataFrame with all AI adoption data\n",
        "    \"\"\"\n",
        "    print(f\"Starting to scrape {len(url_list)} URLs...\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for i, url in enumerate(url_list, 1):\n",
        "        print(f\"\\n[{i}/{len(url_list)}]\", end=\" \")\n",
        "\n",
        "        df = scrape_ai_adoption_from_url(url)\n",
        "        if not df.empty:\n",
        "            all_data.append(df)\n",
        "\n",
        "        # Be respectful to servers\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Combine all data\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        print(f\"\\n\\nTotal adoption rates found: {len(combined_df)}\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"\\n\\nNo adoption data found from any URL\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_adoption_data(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Analyze and display insights from the collected adoption data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing AI adoption data\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(\"No data to analyze\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"AI ADOPTION DATA ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"\\n📊 Basic Statistics:\")\n",
        "    print(f\"   Total adoption rates found: {len(df)}\")\n",
        "    print(f\"   Average adoption rate: {df['adoption_rate'].mean():.1%}\")\n",
        "    print(f\"   Highest adoption rate: {df['adoption_rate'].max():.1%}\")\n",
        "    print(f\"   Lowest adoption rate: {df['adoption_rate'].min():.1%}\")\n",
        "\n",
        "    # By source domain\n",
        "    print(f\"\\n🌐 By Source Domain:\")\n",
        "    domain_stats = df.groupby('source_domain').agg({\n",
        "        'adoption_rate': ['count', 'mean', 'max']\n",
        "    }).round(3)\n",
        "    print(domain_stats)\n",
        "\n",
        "    # By industry\n",
        "    print(f\"\\n�� By Industry:\")\n",
        "    industry_stats = df.groupby('industry').agg({\n",
        "        'adoption_rate': ['count', 'mean', 'max']\n",
        "    }).round(3)\n",
        "    print(industry_stats)\n",
        "\n",
        "    # By year\n",
        "    print(f\"\\n📅 By Year:\")\n",
        "    year_stats = df.groupby('year').agg({\n",
        "        'adoption_rate': ['count', 'mean', 'max']\n",
        "    }).round(3)\n",
        "    print(year_stats)\n",
        "\n",
        "    # Pattern types\n",
        "    print(f\"\\n🔍 Pattern Types Found:\")\n",
        "    pattern_stats = df.groupby('pattern_type').count()['adoption_rate'].sort_values(ascending=False)\n",
        "    print(pattern_stats)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"UNIVERSAL AI ADOPTION DATA SCRAPER\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Example URLs to scrape (you can replace with your own)\n",
        "    urls_to_scrape = [\n",
        "        \"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year\",\n",
        "        \"https://www.gartner.com/en/newsroom/press-releases/2023-08-15-gartner-identifies-four-emerging-technologies-that-will-transform-business-outcomes\",\n",
        "        \"https://www.weforum.org/agenda/2023/01/ai-adoption-business-2023/\",\n",
        "        \"https://octoverse.github.com/2023/state-of-open-source\",\n",
        "        \"https://www.ibm.com/watson/ai-adoption\",\n",
        "        \"https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence-ishtudy.html\"\n",
        "    ]\n",
        "\n",
        "    # Scrape all URLs\n",
        "    adoption_df = scrape_multiple_urls(urls_to_scrape)\n",
        "\n",
        "    if not adoption_df.empty:\n",
        "        # Show sample data\n",
        "        print(f\"\\n📋 Sample Data:\")\n",
        "        print(adoption_df[['source_domain', 'year', 'industry', 'adoption_rate', 'pattern_type']].head(10))\n",
        "\n",
        "        # Analyze the data\n",
        "        analyze_adoption_data(adoption_df)\n",
        "\n",
        "        # Save to CSV\n",
        "        filename = f\"ai_adoption_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        adoption_df.to_csv(filename, index=False)\n",
        "        print(f\"\\n💾 Data saved to: {filename}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n❌ No data collected. Try different URLs or check your internet connection.\")\n",
        "\n",
        "print(\"\\nUniversal AI Adoption Scraper ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izX4yfSUJStg"
      },
      "source": [
        "##### 3.3 Scraping Economic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-USdk26DgsQ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL: Universal Economic Data Scraper\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Universal Economic Data Scraper\n",
        "\n",
        "This script can scrape economic indicators and metrics from any URL by using\n",
        "intelligent pattern matching to detect GDP, inflation, unemployment, income\n",
        "inequality, productivity, and other economic statistics without needing\n",
        "separate parsers for each website.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import urllib.parse\n",
        "\n",
        "def extract_year_from_url_or_content(url: str, soup: BeautifulSoup) -> int:\n",
        "    \"\"\"Extract year from URL or webpage content.\"\"\"\n",
        "    # Try URL first\n",
        "    year_match = re.search(r'20[12]\\d', url)\n",
        "    if year_match:\n",
        "        return int(year_match.group())\n",
        "\n",
        "    # Try page content\n",
        "    text_content = soup.get_text()\n",
        "    year_patterns = [\n",
        "        r'20[12]\\d',\n",
        "        r'published.*20[12]\\d',\n",
        "        r'updated.*20[12]\\d',\n",
        "        r'©.*20[12]\\d'\n",
        "    ]\n",
        "\n",
        "    for pattern in year_patterns:\n",
        "        matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "        if matches:\n",
        "            return int(matches[0])\n",
        "\n",
        "    return datetime.now().year\n",
        "\n",
        "def extract_economic_indicators(text_content: str) -> List[Tuple[str, float, str, str]]:\n",
        "    \"\"\"\n",
        "    Extract economic indicators using comprehensive pattern matching.\n",
        "\n",
        "    Returns:\n",
        "        List of tuples: (indicator_name, value, unit, pattern_used)\n",
        "    \"\"\"\n",
        "    economic_data = []\n",
        "\n",
        "    # Comprehensive patterns for economic indicators\n",
        "    patterns = [\n",
        "        # GDP Patterns\n",
        "        (r'GDP.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|trillion|million)?\\s*(?:dollars?|USD)?', 'gdp', 'dollars'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|trillion|million)?\\s*(?:dollars?|USD).*GDP', 'gdp', 'dollars'),\n",
        "        (r'GDP.*growth.*(\\d+(?:\\.\\d+)?)\\s*percent', 'gdp_growth', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*GDP.*growth', 'gdp_growth', 'percent'),\n",
        "\n",
        "        # Inflation Patterns\n",
        "        (r'inflation.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'inflation_rate', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*inflation', 'inflation_rate', 'percent'),\n",
        "        (r'CPI.*(\\d+(?:\\.\\d+)?)\\s*percent', 'cpi', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*CPI', 'cpi', 'percent'),\n",
        "\n",
        "        # Unemployment Patterns\n",
        "        (r'unemployment.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'unemployment_rate', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*unemployment', 'unemployment_rate', 'percent'),\n",
        "        (r'jobless.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'unemployment_rate', 'percent'),\n",
        "\n",
        "        # Income Inequality Patterns\n",
        "        (r'Gini.*coefficient.*(\\d+(?:\\.\\d+)?)', 'gini_coefficient', 'ratio'),\n",
        "        (r'(\\d+(?:\\.\\d+)?).*Gini.*coefficient', 'gini_coefficient', 'ratio'),\n",
        "        (r'income.*inequality.*(\\d+(?:\\.\\d+)?)\\s*percent', 'income_inequality', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*income.*inequality', 'income_inequality', 'percent'),\n",
        "\n",
        "        # Productivity Patterns\n",
        "        (r'productivity.*growth.*(\\d+(?:\\.\\d+)?)\\s*percent', 'productivity_growth', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*productivity.*growth', 'productivity_growth', 'percent'),\n",
        "        (r'labor.*productivity.*(\\d+(?:\\.\\d+)?)\\s*percent', 'labor_productivity', 'percent'),\n",
        "\n",
        "        # Wage Patterns\n",
        "        (r'average.*wage.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:dollars?|USD)', 'average_wage', 'dollars'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:dollars?|USD).*average.*wage', 'average_wage', 'dollars'),\n",
        "        (r'wage.*growth.*(\\d+(?:\\.\\d+)?)\\s*percent', 'wage_growth', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*wage.*growth', 'wage_growth', 'percent'),\n",
        "\n",
        "        # Interest Rate Patterns\n",
        "        (r'interest.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'interest_rate', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*interest.*rate', 'interest_rate', 'percent'),\n",
        "        (r'federal.*funds.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'federal_funds_rate', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*federal.*funds.*rate', 'federal_funds_rate', 'percent'),\n",
        "\n",
        "        # Employment Patterns\n",
        "        (r'employment.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'employment_rate', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*employment.*rate', 'employment_rate', 'percent'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*jobs.*created', 'jobs_created', 'count'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*jobs.*added', 'jobs_added', 'count'),\n",
        "\n",
        "        # Trade Patterns\n",
        "        (r'trade.*deficit.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD)', 'trade_deficit', 'dollars'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD).*trade.*deficit', 'trade_deficit', 'dollars'),\n",
        "        (r'exports.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD)', 'exports', 'dollars'),\n",
        "        (r'imports.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD)', 'imports', 'dollars'),\n",
        "\n",
        "        # Housing Patterns\n",
        "        (r'median.*home.*price.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:dollars?|USD)', 'median_home_price', 'dollars'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:dollars?|USD).*median.*home.*price', 'median_home_price', 'dollars'),\n",
        "        (r'housing.*starts.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', 'housing_starts', 'count'),\n",
        "\n",
        "        # Consumer Spending Patterns\n",
        "        (r'consumer.*spending.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD)', 'consumer_spending', 'dollars'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD).*consumer.*spending', 'consumer_spending', 'dollars'),\n",
        "\n",
        "        # Business Investment Patterns\n",
        "        (r'business.*investment.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD)', 'business_investment', 'dollars'),\n",
        "        (r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:billion|million)?\\s*(?:dollars?|USD).*business.*investment', 'business_investment', 'dollars'),\n",
        "\n",
        "        # Poverty Patterns\n",
        "        (r'poverty.*rate.*(\\d+(?:\\.\\d+)?)\\s*percent', 'poverty_rate', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*poverty.*rate', 'poverty_rate', 'percent'),\n",
        "\n",
        "        # Wealth Inequality Patterns\n",
        "        (r'wealth.*inequality.*(\\d+(?:\\.\\d+)?)\\s*percent', 'wealth_inequality', 'percent'),\n",
        "        (r'(\\d+(?:\\.\\d+)?)\\s*percent.*wealth.*inequality', 'wealth_inequality', 'percent'),\n",
        "        (r'top.*(\\d+)\\s*percent.*(\\d+(?:\\.\\d+)?)\\s*percent.*wealth', 'top_wealth_share', 'percent'),\n",
        "\n",
        "        # Stock Market Patterns\n",
        "        (r'S&P.*500.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', 'sp500', 'points'),\n",
        "        (r'Dow.*Jones.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', 'dow_jones', 'points'),\n",
        "        (r'NASDAQ.*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', 'nasdaq', 'points'),\n",
        "    ]\n",
        "\n",
        "    for pattern, indicator_name, unit in patterns:\n",
        "        matches = re.findall(pattern, text_content, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                # Clean the value (remove commas, convert to float)\n",
        "                clean_value = match.replace(',', '')\n",
        "                value = float(clean_value)\n",
        "\n",
        "                # Get context\n",
        "                match_index = text_content.lower().find(match.lower())\n",
        "                if match_index != -1:\n",
        "                    start = max(0, match_index - 100)\n",
        "                    end = min(len(text_content), match_index + 100)\n",
        "                    context = text_content[start:end].strip()\n",
        "                else:\n",
        "                    context = \"Context not found\"\n",
        "\n",
        "                economic_data.append((indicator_name, value, unit, context))\n",
        "\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return economic_data\n",
        "\n",
        "def extract_country_from_url_or_content(url: str, soup: BeautifulSoup) -> str:\n",
        "    \"\"\"Extract country information from URL or content.\"\"\"\n",
        "    # Check URL for country keywords\n",
        "    url_lower = url.lower()\n",
        "\n",
        "    country_keywords = {\n",
        "        'usa': 'United States',\n",
        "        'us': 'United States',\n",
        "        'america': 'United States',\n",
        "        'united-states': 'United States',\n",
        "        'uk': 'United Kingdom',\n",
        "        'britain': 'United Kingdom',\n",
        "        'england': 'United Kingdom',\n",
        "        'canada': 'Canada',\n",
        "        'australia': 'Australia',\n",
        "        'germany': 'Germany',\n",
        "        'france': 'France',\n",
        "        'japan': 'Japan',\n",
        "        'china': 'China',\n",
        "        'india': 'India',\n",
        "        'brazil': 'Brazil',\n",
        "        'mexico': 'Mexico'\n",
        "    }\n",
        "\n",
        "    for keyword, country in country_keywords.items():\n",
        "        if keyword in url_lower:\n",
        "            return country\n",
        "\n",
        "    # Check page content\n",
        "    text_content = soup.get_text().lower()\n",
        "    for keyword, country in country_keywords.items():\n",
        "        if keyword in text_content:\n",
        "            return country\n",
        "\n",
        "    return 'Unknown'\n",
        "\n",
        "def scrape_economic_data_from_url(url: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape economic data from any given URL.\n",
        "\n",
        "    Args:\n",
        "        url: The URL to scrape\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing economic data from the URL\n",
        "    \"\"\"\n",
        "    print(f\"Scraping: {url}\")\n",
        "\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove scripts, styles, etc.\n",
        "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        text_content = soup.get_text()\n",
        "\n",
        "        # Extract metadata\n",
        "        year = extract_year_from_url_or_content(url, soup)\n",
        "        country = extract_country_from_url_or_content(url, soup)\n",
        "\n",
        "        # Extract economic indicators\n",
        "        economic_indicators = extract_economic_indicators(text_content)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        economic_data = []\n",
        "        for indicator_name, value, unit, context in economic_indicators:\n",
        "            economic_data.append({\n",
        "                'url': url,\n",
        "                'year': year,\n",
        "                'country': country,\n",
        "                'indicator_name': indicator_name,\n",
        "                'value': value,\n",
        "                'unit': unit,\n",
        "                'context': context[:200] + '...' if len(context) > 200 else context,\n",
        "                'source_domain': urllib.parse.urlparse(url).netloc\n",
        "            })\n",
        "\n",
        "        if economic_data:\n",
        "            print(f\"  Found {len(economic_data)} economic indicator(s)\")\n",
        "        else:\n",
        "            print(f\"  No economic indicators found\")\n",
        "\n",
        "        return pd.DataFrame(economic_data)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  Error: Network issue - {e}\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def scrape_multiple_economic_urls(url_list: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape economic data from a list of URLs.\n",
        "\n",
        "    Args:\n",
        "        url_list: List of URLs to scrape\n",
        "\n",
        "    Returns:\n",
        "        Combined DataFrame with all economic data\n",
        "    \"\"\"\n",
        "    print(f\"Starting to scrape {len(url_list)} URLs for economic data...\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for i, url in enumerate(url_list, 1):\n",
        "        print(f\"\\n[{i}/{len(url_list)}]\", end=\" \")\n",
        "\n",
        "        df = scrape_economic_data_from_url(url)\n",
        "        if not df.empty:\n",
        "            all_data.append(df)\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        print(f\"\\n\\nTotal economic indicators found: {len(combined_df)}\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"\\n\\nNo economic data found from any URL\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_economic_data(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Analyze and display insights from the collected economic data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing economic data\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(\"No data to analyze\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ECONOMIC DATA ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"\\n📊 Basic Statistics:\")\n",
        "    print(f\"   Total indicators found: {len(df)}\")\n",
        "    print(f\"   Unique indicators: {df['indicator_name'].nunique()}\")\n",
        "    print(f\"   Countries covered: {df['country'].nunique()}\")\n",
        "    print(f\"   Years covered: {df['year'].min()} - {df['year'].max()}\")\n",
        "\n",
        "    # By indicator type\n",
        "    print(f\"\\n📈 By Indicator Type:\")\n",
        "    indicator_stats = df.groupby('indicator_name').agg({\n",
        "        'value': ['count', 'mean', 'min', 'max']\n",
        "    }).round(3)\n",
        "    print(indicator_stats)\n",
        "\n",
        "    # By country\n",
        "    print(f\"\\n🌍 By Country:\")\n",
        "    country_stats = df.groupby('country').agg({\n",
        "        'value': ['count', 'mean']\n",
        "    }).round(3)\n",
        "    print(country_stats)\n",
        "\n",
        "    # By year\n",
        "    print(f\"\\n📅 By Year:\")\n",
        "    year_stats = df.groupby('year').agg({\n",
        "        'value': ['count', 'mean']\n",
        "    }).round(3)\n",
        "    print(year_stats)\n",
        "\n",
        "    # By source domain\n",
        "    print(f\"\\n🌐 By Source Domain:\")\n",
        "    domain_stats = df.groupby('source_domain').agg({\n",
        "        'value': ['count', 'mean']\n",
        "    }).round(3)\n",
        "    print(domain_stats)\n",
        "\n",
        "    # Most common indicators\n",
        "    print(f\"\\n🔍 Most Common Indicators:\")\n",
        "    common_indicators = df['indicator_name'].value_counts().head(10)\n",
        "    print(common_indicators)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"UNIVERSAL ECONOMIC DATA SCRAPER\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Example URLs to scrape (you can replace with your own)\n",
        "    urls_to_scrape = [\n",
        "        \"https://www.bls.gov/news.release/empsit.nr0.htm\",  # BLS Employment\n",
        "        \"https://www.bls.gov/news.release/cpi.nr0.htm\",     # BLS Inflation\n",
        "        \"https://www.bea.gov/news/2023/gross-domestic-product-fourth-quarter-and-year-2022-advance-estimate\",  # BEA GDP\n",
        "        \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\",  # Fed rates\n",
        "        \"https://www.census.gov/newsroom/press-releases/2023/income-poverty-health-insurance-coverage.html\",  # Census income\n",
        "        \"https://www.worldbank.org/en/topic/poverty/overview\",  # World Bank poverty\n",
        "        \"https://www.imf.org/en/Publications/WEO/Issues/2023/10/10/world-economic-outlook-october-2023\",  # IMF outlook\n",
        "        \"https://data.oecd.org/gdp/gross-domestic-product-gdp.htm\"  # OECD GDP\n",
        "    ]\n",
        "\n",
        "    # Scrape all URLs\n",
        "    economic_df = scrape_multiple_economic_urls(urls_to_scrape)\n",
        "\n",
        "    if not economic_df.empty:\n",
        "        # Show sample data\n",
        "        print(f\"\\n📋 Sample Data:\")\n",
        "        print(economic_df[['source_domain', 'year', 'country', 'indicator_name', 'value', 'unit']].head(10))\n",
        "\n",
        "        # Analyze the data\n",
        "        analyze_economic_data(economic_df)\n",
        "\n",
        "        # Save to CSV\n",
        "        filename = f\"economic_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        economic_df.to_csv(filename, index=False)\n",
        "        print(f\"\\n💾 Data saved to: {filename}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n❌ No data collected. Try different URLs or check your internet connection.\")\n",
        "\n",
        "print(\"\\nUniversal Economic Data Scraper ready to use!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbHAduihJPm1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xkGcDT_InQl"
      },
      "source": [
        "#### 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqDpW8-KDjkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pu5mT6IIqP4"
      },
      "source": [
        "#### 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY7LE2U9DlWs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
